{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Ref: https://towardsdatascience.com/healthcare-dataset-with-spark-6bf48019892b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Setting up Spark and getting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql as sparksql\n",
    "spark = SparkSession.builder.appName('stroke').getOrCreate()\n",
    "train = spark.read.csv('train_2v.csv', inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Exploration of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// What type of work has more cases of stroke:\n",
    "\n",
    "// Create DataFrame as a temporary view\n",
    "train.createOrReplaceTempView('table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// SQL\n",
    "spark.sql(\"SELECT work_type, count(work_type) as work_type_count\n",
    "FROM table WHERE stroke == 1 GROUP BY work_type ORDER BY\n",
    "work_type_count DESC\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// who participated in this clinic measurement\n",
    "spark.sql(\"SELECT gender, count (gender) as count_gender,\n",
    "count(gender)*100/sum(count(gender)) over() as percent FROM table\n",
    "GROUP BY gender\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// retrieve information about how many Female/Male have a stroke\n",
    "spark.sql(\"SELECT gender, count(gender), (COUNT(gender) * 100.0)\n",
    "/(SELECT count(gender) FROM table WHERE gender == 'Male') as\n",
    "percentage FROM table WHERE stroke = '1' and gender = 'Male' GROUP\n",
    "BY gender\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT gender, count(gender), (COUNT(gender) * 100.0)\n",
    "/(SELECT count(gender) FROM table WHERE gender == 'Female') as\n",
    "percentage FROM table WHERE stroke = '1' and gender = 'Female' GROUP\n",
    "BY gender\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// see if the age has an influence on stroke and what is the risk by age\n",
    "spark.sql(\"SELECT age, count(age) as age_count FROM table WHERE\n",
    "stroke == 1 GROUP BY age ORDER BY age_count DESC\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// use filter operation to calculate the number of stroke cases for people after 50 years\n",
    "train.filter((train['stroke'] == 1) & (train['age'] > '50')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// fill in missing values\n",
    "train_f = train.na.fill('No Info', subset=['smoking_status'])\n",
    "\n",
    "// fill in miss values with mean\n",
    "from pyspark.sql.functions import mean\n",
    "mean = train_f.select(mean(train_f['bmi'])).collect()\n",
    "mean_bmi = mean[0][0]\n",
    "train_f = train_f.na.fill(mean_bmi,['bmi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// The encoding allows algorithms which expect continuous features to use categorical features.\n",
    "// StringIndexer -> OneHotEncoder -> VectorAssembler\n",
    "from pyspark.ml.feature import (VectorAssembler,OneHotEncoder,\n",
    "                                StringIndexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_indexer = StringIndex(inputCol = 'gender', outputCol = 'genderIndex')\n",
    "gender_encoder = OneHotEncoder(inputCol = 'genderIndex', outputCol = 'genderVec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ever_married_indexer = StringIndex(inputCol = 'ever_married', outputCol = 'ever_marriedIndex')\n",
    "ever_married_encoder = OneHotEncoder(inputCol = 'ever_marriedIndex', outputCol = 'ever_marriedVec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Residence_type_indexer = StringIndex(inputCol = 'Residence_type', outputCol = 'Residence_typeIndex')\n",
    "Residence_type_encoder = OneHotEncoder(inputCol = 'Residence_typeIndex', outputCol = 'Residence_typeVec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoking_status_indexer = StringIndex(inputCol = 'smoking_status', outputCol = 'smoking_statusIndex')\n",
    "smoking_status_encoder = OneHotEncoder(inputCol = 'smoking_statusIndex', outputCol = 'smoking_statusVec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Create an assembler, that combines a given list of columns \n",
    "// into a single vector column to train ML model. \n",
    "\n",
    "// Author uses the vector columns, that we got after one_hot_encoding.\n",
    "\n",
    "assembler = VectorAssembler(inputCols=['genderVec',\n",
    " 'age',\n",
    " 'hypertension',\n",
    " 'heart_disease',\n",
    " 'ever_marriedVec',\n",
    " 'work_typeVec',\n",
    " 'Residence_typeVec',\n",
    " 'avg_glucose_level',\n",
    " 'bmi',\n",
    " 'smoking_statusVec'],outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Create a DecisionTree object\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "dtc = DecisionTreeClassifier(labelCol='stroke',featuresCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// To wrap all of that Spark ML represents such a workflow as a Pipeline, \n",
    "// which consists of a sequence of PipelineStages to be run in a specific order.\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[gender_indexer, ever_married_indexer, work_type_indexer, Residence_type_indexer,\n",
    "                           smoking_status_indexer, gender_encoder, ever_married_encoder, work_type_encoder,\n",
    "                           Residence_type_encoder, smoking_status_encoder, assembler, dtc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// split dataset to train and test\n",
    "train_data,test_data = train_f.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//  Fit the model: Author uses the pipeline that was created and train_data\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "// transform the test_data\n",
    "dtc_predictions = model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// evaluate a model\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "# Select (prediction, true label) and compute test error\n",
    "acc_evaluator = MulticlassClassificationEvaluator(labelCol=\"stroke\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "dtc_acc = acc_evaluator.evaluate(dtc_predictions)\n",
    "print('A Decision Tree algorithm had an accuracy of: {0:2.2f}%'.format(dtc_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Note: A Decision Tree algorithm had an accuracy of: 98.08%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
